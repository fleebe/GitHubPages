{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06b54d4d-3fa0-4c52-a32d-e8e4d93897f9",
   "metadata": {},
   "source": [
    "# Langchain Agents\n",
    "\n",
    "[LangChain Agent](https://docs.langchain.com/oss/python/langchain/agents)\n",
    "\n",
    "Combine LLM with tools to create systems that can reason about tasks decide which tools to use and iteratively work toward solutions. Basically achieve intelligent responses to requests.\n",
    "\n",
    "A flow (multi-step) that is controlled/defined by calls to an LLM. Allows tools (web access, DB access, humans) to be called before and after LLM.\n",
    "\n",
    "Kinds of agent\n",
    "- routers (simple) low control from LLM\n",
    "- fully autonomous or select based on reasoning a step, can generate its own next move. Reliability drops.\n",
    "\n",
    "![Agent Control Level](./AgentControlLevel.png)\n",
    "\n",
    "Agents run in a loop to achieve a goal until a stop condition is met, a final output is emitted or iteration limit reached.\n",
    "\n",
    "![Agent Loop](./AgentLoop.png)\n",
    "\n",
    "\n",
    "## Features\n",
    "\n",
    "* [Dynamic Model Selection](#Dynamic-Model-Selection)\n",
    "* [Tool Calling](#tool-calling)\n",
    "* [Middleware](#middleware) - used to\n",
    "* Dynamic Prompts\n",
    "* Structured Outputs\n",
    "* Memory\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687212c5-1c40-423d-b8b9-7767fa1ffe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_lib.settings import settings  # library that reads settings from a json settings file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_chat(model_name: str | None) -> BaseChatModel:\n",
    "    \"\"\"Return the chat model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model to use. If None, use default from settings.json\n",
    "    \"\"\"\n",
    "    if not model_name:\n",
    "        llm_model = settings.llm_settings.get('model')\n",
    "        return init_chat_model(\n",
    "            model=f\"ollama:{llm_model.split('ollama:')[1]}\",\n",
    "            temperature=0.0,\n",
    "            timeout=10\n",
    "        )\n",
    "    else:\n",
    "        return init_chat_model(\n",
    "            model=model_name,\n",
    "            temperature=0.0,\n",
    "            timeout=10\n",
    "        )\n",
    "\n",
    "# using model instance\n",
    "model = get_chat(\"ollama:qwen3-coder:30b\")\n",
    "agent = create_agent(model=model, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344d175-abb2-4582-956b-1085551cf147",
   "metadata": {},
   "source": [
    "## Dynamic Model Selection\n",
    "\n",
    "Some models might be good for functionality like tool calling and others good for reasoning or summarization.\n",
    "\n",
    "### Method\n",
    "\n",
    "This is done in middleware from `import langchain.agents.middleware` using a `@wrap_model_call` decorator. When the model is created the middleware function is massed. `middleware=[dynamic_model_selection]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a6600-aa74-41b7-b420-dd71ef21992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    return handler(request.override(model=model))\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    tools=tools,\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b38b29-e423-462d-b7cf-c0eb33489335",
   "metadata": {},
   "source": [
    "## Tool Calling\n",
    "\n",
    "These give the agent the ability to take actions like:\n",
    "\n",
    "* access a database\n",
    "* do a web search\n",
    "* access an API\n",
    "\n",
    "### Method\n",
    "A function list is passed to `create_agent` function. The functions are decoorated with the `@tool` decorator from `import langchain.tools import tool`.\n",
    "\n",
    "Error handling is done by middleware and `@wrap_tool_call` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76a28a-dabf-4c7b-a2ce-5891367ca273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    return f\"Weather in {location}: Sunny, 72°F\"\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model, # can be the name of a model or an instance\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973b099-4e5f-46bf-abf5-0c78f40e3f38",
   "metadata": {},
   "source": [
    "## Middleware\n",
    "\n",
    "Enables the customization of agent behaviour. Used to\n",
    "\n",
    "* Process state before the model is called (e.g., message trimming, context injection)\n",
    "* Modify or validate the model’s response (e.g., guardrails, content filtering)\n",
    "* Handle tool execution errors with custom logic\n",
    "* Implement dynamic model selection based on state or context\n",
    "* Add custom logging, monitoring, or analytics\n",
    "\n",
    "### Hooks\n",
    "\n",
    "Specified with a decorator at specific execution points.\n",
    "* `before_agent` (once per invocation)\n",
    "* `after_agent`\n",
    "* `before_model`\n",
    "* `after_model` (once per invocation)\n",
    "\n",
    "- `wrap_model_call` - Around each model call\n",
    "- `wrap_tool_call` - Around each tool call\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
